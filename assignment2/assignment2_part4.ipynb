{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89d7da2ffe5499ff359e00c99de3a86b",
     "grade": false,
     "grade_id": "cell-ea66a0e8e4782328",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"v2.2.033020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dcf8a82f573f5c3ef31d4b9eefa55d78",
     "grade": false,
     "grade_id": "cell-d830818b078f1db4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: Mining Itemsets (Part IV)\n",
    "\n",
    "## Evaluating Frequent Itemsets\n",
    "\n",
    "Even though we have found all the frequent itemsets, not all of them are interesting. In Part IV of this assignment, we will practice how to evaluate the frequent itemsets.\n",
    "\n",
    "First, let's import the packages and dependencies that will be used in this part of assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5680034bd4b3c02e549d03c8eb3453a",
     "grade": false,
     "grade_id": "cell-ac6925b7e40642d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "432e032c62ce4be4b6905d608924cf23",
     "grade": false,
     "grade_id": "cell-8af76ec85c41ac02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**<span style=\"color:red\">NOTE: These are all the imports we need to make for this assignment (Part IV). You should not make other imports in your submitted notebook. You will receive 0 points for the exercises if your solution includes additional imports.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fcc32e248131bac705418405544183d",
     "grade": false,
     "grade_id": "cell-0c6f73e5d02aa167",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "People have developed various measurements of the interestingness of patterns. Most of them split the itemset into an antecedent item(set) and a consequent item(set), and then measure the correlation between the antecedent and the consequent. Let's try some of such measurements implemented by the `mlxtend.frequent_patterns.association_rules` API, which we have imported. For more information about the API, visit the [documentation](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/) of the `mlxtend` package.\n",
    "\n",
    "Let's again use the shopping basket dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c38bca49fce067364e49e3fbc1e892e",
     "grade": false,
     "grade_id": "cell-f0395971d14c6da8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "market_df = pd.read_csv('assets/shopping_basket.csv')\n",
    "market_frequent_itemsets = apriori(market_df, min_support=0.005, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b3a7b87582ad4b154a3f5dda66c9f19",
     "grade": false,
     "grade_id": "cell-6ade5824de0b915c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "interestingness_measurements = association_rules(market_frequent_itemsets, metric=\"lift\", min_threshold=0)\n",
    "interestingness_measurements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66753fc8fef3aa67646ddeae4d3458ef",
     "grade": false,
     "grade_id": "cell-2eb88bf08fb726e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In the returned data frame, each row examines one (antecedent -> consequent) pair. *Antecedent support* and *consequent support* measure *P*(antecedent) and *P*(consequent), while *support* measures *P*(antecedent, consequent). In fact, these three values help us characterize the $2\\times2$ contingency table, as illustrated in the following table:\n",
    " \n",
    " |           |              |       |              | \n",
    " -----------:|:------------:|-------|---------------\n",
    " |           |    X = 1     | X = 0 |   sum(row)   |\n",
    " |     Y = 1 |   `support`    |       | `cons_support` |\n",
    " |     Y = 0 |              |       |              |\n",
    " | sum(col.) | `ante_support` |       |              |\n",
    "\n",
    "Most interestingness measurements, including the four shown in the data frame (*confidence*, *lift*, *leverage*, and *conviction*), can be derived from the three support values. For example, $$\\text{confidence}=\\frac{\\text{support}}{\\text{antecedent_support}},$$ and $$\\text{lift} =\\frac{\\text{confidence}}{\\text{consequent_support}}=\\frac{\\text{support}}{\\text{antecedent_support} * \\text{consequent_support}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85d2af76780ccb7d6929ab42b41ad036",
     "grade": false,
     "grade_id": "cell-27fbe9956d8ea59f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 5. (15 pts)\n",
    "In this exercise, we are going to implement another interestingness measurement, the (full) mutual information, and add a 'mutual information' column to the data frame. The measurement is defined as\n",
    "\n",
    "$$I(X;Y)=\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}} P(X=x, Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(P=y)}.$$\n",
    "\n",
    "Note that the logorithm requirest that the joint probability $P(X=x, Y=y) > 0$, which does not hold for some $(x, y)$. However, since we know that when $P(X=x, Y=y) = 0$, it would not contribute to the sum, you may assume $P(X=x, Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(P=y)} = 0$ in that case. \n",
    "\n",
    "$x$, $y$ are possible values of $X$ and $Y$; in the case of appearance or absence of an item, 1 or 0. Therefore, we need to consider all possible combinations of $x$ and $y$, that is, $(X=1, Y=1)$, $(X=1, Y=0)$, $(X=0, Y=1)$, $(X=0, Y=0)$.\n",
    "\n",
    "Please complete the following function that uses the three support values to compute the mutual information. All the three parameters are in [0, 1], and you can assume the validity of the input. **Use 2 as the log base.** We have created some auxilary variables for you, each represent a joint or marginal (let $X$ denote the antecedent item and $Y$ denote the consequent item) probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "56f7d970830040f81cb4dbb169d840d0",
     "grade": false,
     "grade_id": "cell-bb65dd0618e121e4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mi(antecedent_support, consequent_support, support):\n",
    "       \n",
    "    px1 = antecedent_support\n",
    "    px0 = 1 - antecedent_support\n",
    "    py1 = consequent_support\n",
    "    py0 = 1 - consequent_support\n",
    "    \n",
    "    px1y1 = support\n",
    "    px1y0 = px1 - px1y1\n",
    "    px0y1 = py1 - px1y1\n",
    "    px0y0 = 1 - px1 - py1 + px1y1\n",
    "    \n",
    "    mutual_information = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mutual_information      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2efd88c49d4b64a721042fd4bf593418",
     "grade": true,
     "grade_id": "cell-6c50e1efca417fcb",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This code block tests whether the `mi` function work as expected.\n",
    "# We hide some tests, so passing all the displayed assertions does not guarantee the bonus points.\n",
    "\n",
    "assert np.abs(mi(0.6, 0.75, 0.4) - 0.04287484674660057) < 1e-8\n",
    "assert np.abs(mi(0.5, 0.5, 0.25) - 0) < 1e-8\n",
    "\n",
    "# If you fail the following assertion, double check if your function \n",
    "# handles the scenarios in which a joint probability is zero.\n",
    "assert np.abs(mi(0.5, 0.5, 0.5) - 1) < 1e-8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d8f729b2537beed60d0a2443ae7200d",
     "grade": false,
     "grade_id": "cell-1770a108fe067333",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Mutual information is a classical measure of interestingness. We encourage you to think about the following questions (not graded):\n",
    "1. What is the maximum of mutual information in this setting? How to reach it?\n",
    "2. What is the minimum of mutual information in this setting? How to reach it?\n",
    "\n",
    "What does the max/min value imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fbc3c3bdf9fca93524dfa900efcc88c",
     "grade": false,
     "grade_id": "cell-03be28a9dfec2fce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "With the `mi` function, we can now compute the mutual information for each (antecedent -> consequent) pair and attach it to the data frame. Does the result make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06332f330a9c4062e824c0985f5c584e",
     "grade": false,
     "grade_id": "cell-881f60162f4bef12",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "interestingness_measurements['mi'] = \\\n",
    "    interestingness_measurements.apply(lambda pair: mi(pair['antecedent support'], \n",
    "                                              pair['consequent support'], \n",
    "                                              pair['support']),\n",
    "                                       axis=1)\n",
    "interestingness_measurements.sort_values('mi', ascending=False).head(n=5)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_i_v2_assignment2_part4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
